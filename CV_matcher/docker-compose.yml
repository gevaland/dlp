version: '3.4'

services:
  fast_api:
    # image: python:3.10-slim
    build:
      context: .
      dockerfile: ./fast_api/Dockerfile
    ports:
      - 8600:8600
    network_mode: "host"
    volumes:
      - ./fast_api/storage:/app/storage
    command: "uvicorn fast_api.main:app --port 8600"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '0' ]
              capabilities: [ gpu ]
  postgres:
    image: postgres:13.3
    environment:
      POSTGRES_DB: cv_match_db
      POSTGRES_USER: cv_match_user
      POSTGRES_PASSWORD: cv_match_password
    ports:
      - "5432:5432"
    network_mode: "host"
  inference_server:
    image: nvcr.io/nvidia/tritonserver:23.08-py3
    # runtime: nvidia

    volumes:
      - ./triton_model_repo/:/models
    ports:
      - 8400:8000
      - 8401:8001
      - 8402:8002
    command: [ "tritonserver", "--model-repository=/models" ]
    shm_size: 4g
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '0' ]
              capabilities: [ gpu ]
    ulimits:
      memlock: -1
      stack: 67108864
    network_mode: "host"
  streamlit:
    build:
      context: .
      dockerfile: ./streamlit/Dockerfile
    volumes:
      - ./streamlit:/app/streamlit
      - ./fast_api/storage/:/app/storage
    command: "streamlit run --server.port 8501 streamlit/main.py"
    ports:
      - "8501:8501"
    network_mode: "host"
