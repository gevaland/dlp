# -*- coding: utf-8 -*-
"""nlp_task_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v7R7zjljmo4SBvfosc1FIqTCfytup0XI

Dataset from https://github.com/natasha/corus
"""


"""Fix random state"""

import random

random.seed(42)



# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""## Loading the dataset"""

!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.1/lenta-ru-news.csv.bz2

import corus



def load_lenta_to_list(path, max_number=None):
    records = corus.load_lenta2(path)
    texts, titles = [], []
    for i, record in enumerate(records):
        texts.append(record.text)
        titles.append(record.title)
        if not max_number is None and i >= max_number-1:
            break
    return texts, titles

texts, titles = load_lenta_to_list('lenta-ru-news.csv.bz2', max_number=10)
len(texts), len(titles)

df = pd.DataFrame({'text':texts, 'title':titles})
df.head()

"""## Checking the dataset structure and stats"""

import nltk
from nltk import corpus, tokenize, word_tokenize, sent_tokenize

nltk.download('punkt')
nltk.download('stopwords')

word_only_tokenize = tokenize.RegexpTokenizer(r'\w+').tokenize

df['sent_tokens'] = df['text'].apply(sent_tokenize)
df['word_tokens'] = df['text'].apply(word_tokenize)

df.head()

"""## Preprocessing the data inside"""

average_word_tokens_count = round(df['word_tokens'].apply(len).sum()/df['sent_tokens'].count())
unique_word_tokens_count = df['word_tokens'].apply(len).sum()

average_word_tokens_count, unique_word_tokens_count

df['word_only_tokens'] = df['text'].apply(word_only_tokenize)
df['lowered_word_only_tokens'] = df['word_only_tokens'].apply(lambda wl: map(lambda w: w.lower(), wl))
df['lowered_unique_word_only_tokens'] = df['lowered_word_only_tokens'].apply(lambda wl: set(wl))

ru_stop_words = corpus.stopwords.words('russian')
df['lowered_unique_no_stop_word_only_tokens'] = df['lowered_unique_word_only_tokens'].apply(lambda ws: ws - set(ru_stop_words))

lowered_unique_word_only_tokens_count = df['lowered_unique_word_only_tokens'].apply(len).sum()
lowered_unique_no_stop_word_only_tokens_count = df['lowered_unique_no_stop_word_only_tokens'].apply(len).sum()

lowered_unique_word_only_tokens_count, lowered_unique_no_stop_word_only_tokens_count

from pymystem3 import Mystem

mst = Mystem()
def lemmatize(words):
    words_result = []
    for word in words:
        words_result.append(mst.lemmatize(word)[0])
    return words_result

df['lowered_unique_word_only_tokens_lemmas'] = df['lowered_unique_word_only_tokens'].apply(lemmatize)
df['lowered_unique_word_only_tokens_lemmas'].head()

from pymorphy2 import MorphAnalyzer

man = MorphAnalyzer()
def morph(words):
    words_result = []
    for word in words:
        words_result.append(man.parse(word)[0].word)
    return words_result

df['lowered_unique_word_only_tokens_morphs'] = df['lowered_unique_word_only_tokens'].apply(morph)
df['lowered_unique_word_only_tokens_morphs'].head()